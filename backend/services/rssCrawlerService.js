const Parser = require('rss-parser')
const { supabaseAdmin } = require('../config/supabase')
const { analyzeWithAI } = require('./aiAnalysisService')

const parser = new Parser({
  customFields: {
    item: ['media:content', 'content:encoded', 'dc:creator']
  },
  headers: {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
  },
  timeout: 10000
})

const RSS_SOURCES = {
  critical: [
    { name: 'TechCrunch AI', url: 'https://techcrunch.com/category/artificial-intelligence/feed/', category: 'news' },
    { name: 'OpenAI Blog', url: 'https://openai.com/blog/rss.xml', category: 'company' },
  ],
  important: [
    // VentureBeat ì œê±° (429 ì—ëŸ¬)
    { name: 'The Verge AI', url: 'https://www.theverge.com/rss/ai/index.xml', category: 'news' },
    { name: 'MIT Tech Review', url: 'https://www.technologyreview.com/feed/', category: 'research' },
  ],
  korean: [
    // AIíƒ€ì„ìŠ¤ ì œê±° (403 ì—ëŸ¬)
    { name: 'Naver AI NOW', url: 'https://naver.github.io/feed.xml', category: 'korean' },
  ]
}

async function crawlRSS(source) {
  try {
    console.log(`ğŸ“¡ í¬ë¡¤ë§ ì‹œì‘: ${source.name}`)
    
    // íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ RSS íŒŒì‹±
    const feed = await Promise.race([
      parser.parseURL(source.url),
      new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Timeout')), 10000)
      )
    ])
    
    if (!feed || !feed.items || feed.items.length === 0) {
      console.log(`âš ï¸ ${source.name}: ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤`)
      return []
    }
    
    const items = []
    for (const item of feed.items.slice(0, 5)) { // ìµœê·¼ 5ê°œë§Œ (ë¶€í•˜ ê°ì†Œ)
      try {
        // ì¤‘ë³µ ì²´í¬
        const { data: existing } = await supabaseAdmin
          .from('products')
          .select('id')
          .eq('original_url', item.link)
          .single()
        
        if (existing) {
          console.log(`â­ï¸ ì´ë¯¸ ì¡´ì¬: ${item.title}`)
          continue
        }
        
        // AI ë¶„ì„
        const analysis = await analyzeWithAI({
          title: item.title,
          content: item.content || item.contentSnippet || '',
          url: item.link,
          source: source.name
        })
        
        // DBì— ì €ì¥
        const trendItem = {
          title: item.title,
          original_url: item.link,
          source: source.name,
          published_at: new Date(item.pubDate || item.isoDate),
          category: source.category,
          ...analysis
        }
        
        const { error } = await supabaseAdmin
          .from('products')
          .insert([trendItem])
        
        if (error) {
          console.error(`âŒ ì €ì¥ ì‹¤íŒ¨: ${item.title}`, error)
        } else {
          console.log(`âœ… ì €ì¥ ì™„ë£Œ: ${item.title}`)
          items.push(trendItem)
        }
        
      } catch (itemError) {
        console.error(`âŒ ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: ${item.title}`, itemError)
      }
    }
    
    return items
  } catch (error) {
    console.error(`âŒ RSS í¬ë¡¤ë§ ì‹¤íŒ¨: ${source.name}`, error)
    return []
  }
}

async function startRSSCrawler() {
  console.log('ğŸš€ RSS í¬ë¡¤ëŸ¬ ì‹œì‘...')
  
  const allSources = [
    ...RSS_SOURCES.critical,
    ...RSS_SOURCES.important,
    ...RSS_SOURCES.korean
  ]
  
  // ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ rate limit íšŒí”¼
  const results = []
  for (const source of allSources) {
    try {
      const items = await crawlRSS(source)
      results.push({ status: 'fulfilled', value: items })
      
      // ê° ì†ŒìŠ¤ ê°„ 2ì´ˆ ë”œë ˆì´
      await new Promise(resolve => setTimeout(resolve, 2000))
    } catch (error) {
      results.push({ status: 'rejected', reason: error })
    }
  }
  
  const totalItems = results
    .filter(r => r.status === 'fulfilled')
    .reduce((sum, r) => sum + r.value.length, 0)
  
  console.log(`âœ… RSS í¬ë¡¤ë§ ì™„ë£Œ: ì´ ${totalItems}ê°œ ì•„ì´í…œ ì €ì¥ë¨`)
}

module.exports = {
  startRSSCrawler,
  crawlRSS
}